{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525583505
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#conda install azure-common azure-ai-ml==0.1.0b7 mltable==0.1.0b4 azureml_dataprep azureml_dataprep_rslex responsibleai raiwidgets pandas pyarrow shap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525584631
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525585414
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#from raiwidgets import ResponsibleAIDashboard\n",
    "#from responsibleai import RAIInsights\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525587440
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.entities import Environment, BuildContext\n",
    "from azureml.mlflow import register_model\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "#connect to the workspace\n",
    "registry_name = \"azureml\"\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client =  MLClient.from_config(credential=credential)\n",
    "\n",
    "ml_client_registry = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=ml_client.subscription_id,\n",
    "    resource_group_name=ml_client.resource_group_name,\n",
    "    registry_name=registry_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525587640
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "compute_name = \"ResponsibleAI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525588080
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "all_compute_names = [x.name for x in ml_client.compute.list()]\n",
    "\n",
    "if compute_name in all_compute_names:\n",
    "    print(f\"Found existing compute: {compute_name}\")\n",
    "else:\n",
    "    my_compute = AmlCompute(\n",
    "        name=compute_name,\n",
    "        size=\"STANDARD_DS12_V2\",\n",
    "        min_instances=0,\n",
    "        max_instances=2,\n",
    "        idle_time_before_scale_down=3600\n",
    "    )\n",
    "    ml_client.compute.begin_create_or_update(my_compute)\n",
    "    print(\"Initiated compute creation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525588356
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "rai_hospital_classifier_version_string = '1'\n",
    "version='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525588570
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_parquet('data/train_dataset.parquet')\n",
    "test_data = pd.read_parquet('data/test_dataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525588933
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "target_column = \"readmit_status\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525589817
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "display(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525590055
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(train_data[target_column].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525590282
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(test_data[target_column].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525590513
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525591032
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "\n",
    "training_dataset_filename = 'hospital_train_parquet'\n",
    "testing_dataset_filename = 'hospital_test_parquet'\n",
    "\n",
    "\n",
    "training_data = Data(\n",
    "    name=training_dataset_filename,\n",
    "    path='data/train_dataset.parquet',\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"RAI hospital  train data\",  \n",
    ")\n",
    "\n",
    "tr_data = ml_client.data.create_or_update(training_data)\n",
    "\n",
    "testing_data = Data(\n",
    "    name=testing_dataset_filename,\n",
    "    path='data/test_dataset.parquet',\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"RAI hospital  test data\",  \n",
    ")\n",
    "\n",
    "te_data = ml_client.data.create_or_update(testing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525591510
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('component', exist_ok=True)\n",
    "os.makedirs('register_model_src', exist_ok=True)\n",
    "os.makedirs('environment', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%writefile component/hospital_training.py\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir =  os.path.dirname(os.getcwd())\n",
    " \n",
    "# setting path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--training_data\", type=str, help=\"Path to training data\")\n",
    "    parser.add_argument(\"--target_column_name\", type=str, help=\"Name of target column\")\n",
    "    parser.add_argument(\"--model_output\", type=str, help=\"Path of output model\")\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()    \n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "def get_categorical_index(categorical_fields):\n",
    "    cat_idx = []\n",
    "    for col, value in categorical_fields.iteritems():\n",
    "        if value.dtype == 'object':\n",
    "            cat_idx.append(categorical_fields.columns.get_loc(col))\n",
    "    print(\"col indices: \", cat_idx)  \n",
    "    return cat_idx    \n",
    "\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    current_experiment = Run.get_context().experiment\n",
    "    tracking_uri = current_experiment.workspace.get_mlflow_tracking_uri()\n",
    "    print(\"tracking_uri: {0}\".format(tracking_uri))\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "    mlflow.set_experiment(current_experiment.name)\n",
    "\n",
    "    # Read in data\n",
    "    print(\"Reading data\")\n",
    "    all_training_data = pd.read_parquet(args.training_data)\n",
    "    target = all_training_data[args.target_column_name]\n",
    "    features = all_training_data.drop([args.target_column_name], axis = 1)  \n",
    "\n",
    "    #features['age'] = pd.Categorical(all_training_data['age'], categories=['30 years or younger', '30-60 years', 'Over 60 years'], ordered=True)\n",
    "\n",
    "    # Transform string data to numeric\n",
    "    numerical_selector = selector(dtype_include=np.number)\n",
    "    categorical_selector = selector(dtype_exclude=np.number)\n",
    "\n",
    "    numerical_columns = numerical_selector(features)\n",
    "    categorical_columns = categorical_selector(features)\n",
    "\n",
    "    categorial_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    numerical_encoder = StandardScaler()\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "    ('categorical-encoder', categorial_encoder, categorical_columns),\n",
    "    ('standard_scaler', numerical_encoder, numerical_columns)])\n",
    "\n",
    "    categorical_indices = get_categorical_index(features)\n",
    "    #clf = make_pipeline(preprocessor, RandomForestClassifier())\n",
    "    clf = make_pipeline(preprocessor, LogisticRegression())\n",
    "    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=1)\n",
    "\n",
    "    print(\"Training model...\") \n",
    "    \n",
    "    model = clf.fit(X_train, y_train)\n",
    " \n",
    "    # Saving model with mlflow - leave this section unchanged\n",
    "    model_dir =  \"./model_output\"\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        print(\"Saving model with MLFlow to temporary directory\")\n",
    "        tmp_output_dir = os.path.join(td, model_dir)\n",
    "        mlflow.sklearn.save_model(sk_model=model, path=tmp_output_dir)\n",
    "\n",
    "        print(\"Copying MLFlow model to output path\")\n",
    "        for file_name in os.listdir(tmp_output_dir):\n",
    "            print(\"  Copying: \", file_name)\n",
    "            # As of Python 3.8, copytree will acquire dirs_exist_ok as\n",
    "            # an option, removing the need for listdir\n",
    "            shutil.copy2(src=os.path.join(tmp_output_dir, file_name), dst=os.path.join(args.model_output, file_name))\n",
    "\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)\n",
    "\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%writefile register_model_src/model_register.py\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Based on example:\n",
    "# https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-cli\n",
    "# which references\n",
    "# https://github.com/Azure/azureml-examples/tree/main/cli/jobs/train/lightgbm/iris\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--model_input_path\", type=str, help=\"Path to input model\")\n",
    "    parser.add_argument(\n",
    "        \"--model_info_output_path\", type=str, help=\"Path to write model info JSON\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_base_name\", type=str, help=\"Name of the registered model\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_suffix\", type=int, help=\"Set negative to use epoch_secs\"\n",
    "    )\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    current_experiment = Run.get_context().experiment\n",
    "    tracking_uri = current_experiment.workspace.get_mlflow_tracking_uri()\n",
    "    print(\"tracking_uri: {0}\".format(tracking_uri))\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "    mlflow.set_experiment(current_experiment.name)\n",
    "\n",
    "    print(\"Loading model\")\n",
    "    mlflow_model = mlflow.sklearn.load_model(args.model_input_path)\n",
    "\n",
    "    if args.model_name_suffix < 0:\n",
    "        suffix = int(time.time())\n",
    "    else:\n",
    "        suffix = args.model_name_suffix\n",
    "    registered_name = \"{0}_{1}\".format(args.model_base_name, suffix)\n",
    "    print(f\"Registering model as {registered_name}\")\n",
    "\n",
    "    print(\"Registering via MLFlow\")\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=mlflow_model,\n",
    "        registered_model_name=registered_name,\n",
    "        artifact_path=registered_name,\n",
    "    )\n",
    "\n",
    "    print(\"Writing JSON\")\n",
    "    dict = {\"id\": \"{0}:1\".format(registered_name)}\n",
    "    output_path = os.path.join(args.model_info_output_path, \"model_info.json\")\n",
    "    with open(output_path, \"w\") as of:\n",
    "        json.dump(dict, fp=of)\n",
    "\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)\n",
    "\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525614411
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import load_component\n",
    "\n",
    "yaml_contents = f\"\"\"\n",
    "$schema: http://azureml/sdk-2-0/CommandComponent.json\n",
    "name: rai_hospital_training_component\n",
    "display_name: hospital  classification training component for RAI example\n",
    "version: {rai_hospital_classifier_version_string}\n",
    "type: command\n",
    "inputs:\n",
    "  training_data:\n",
    "    type: path\n",
    "  target_column_name:\n",
    "    type: string\n",
    "outputs:\n",
    "  model_output:\n",
    "    type: path\n",
    "code: ./component/\n",
    "environment: azureml://registries/azureml/environments/responsibleai-ubuntu20.04-py38-cpu/versions/42\n",
    "\"\"\" + r\"\"\"\n",
    "command: >-\n",
    "  python hospital_training.py\n",
    "  --training_data ${{{{inputs.training_data}}}}\n",
    "  --target_column_name ${{{{inputs.target_column_name}}}}\n",
    "  --model_output ${{{{outputs.model_output}}}}\n",
    "\"\"\"\n",
    "\n",
    "yaml_filename = \"RAIhospitalClassificationTrainingComponent.yaml\"\n",
    "\n",
    "with open(yaml_filename, 'w') as f:\n",
    "    f.write(yaml_contents.format(yaml_contents))\n",
    "    \n",
    "train_component_definition = load_component(\n",
    "    source=yaml_filename\n",
    ")\n",
    "\n",
    "ml_client.components.create_or_update(train_component_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525616741
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "yaml_contents = f\"\"\"\n",
    "$schema: http://azureml/sdk-2-0/CommandComponent.json\n",
    "name: register_hospital_model\n",
    "display_name: Register hospital Model\n",
    "version: {rai_hospital_classifier_version_string}\n",
    "type: command\n",
    "is_deterministic: False\n",
    "inputs:\n",
    "  model_input_path:\n",
    "    type: path\n",
    "  model_base_name:\n",
    "    type: string\n",
    "  model_name_suffix: # Set negative to use epoch_secs\n",
    "    type: integer\n",
    "    default: -1\n",
    "outputs:\n",
    "  model_info_output_path:\n",
    "    type: path\n",
    "code: ./register_model_src/\n",
    "environment: azureml://registries/azureml/environments/responsibleai-ubuntu20.04-py38-cpu/versions/42\n",
    "command: >-\n",
    "  python model_register.py\n",
    "  --model_input_path ${{{{inputs.model_input_path}}}}\n",
    "  --model_base_name ${{{{inputs.model_base_name}}}}\n",
    "  --model_name_suffix ${{{{inputs.model_name_suffix}}}}\n",
    "  --model_info_output_path ${{{{outputs.model_info_output_path}}}}\n",
    "\n",
    "\"\"\"\n",
    "yaml_filename = \"model_register.yaml\"\n",
    "\n",
    "with open(yaml_filename, 'w') as f:\n",
    "    f.write(yaml_contents)\n",
    "    \n",
    "register_component = load_component(\n",
    "    source=yaml_filename\n",
    ")\n",
    "\n",
    "ml_client.components.create_or_update(register_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525616947
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model_name_suffix = int(time.time())\n",
    "model_base_name = 'rai_hospital_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525617215
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import dsl, Input\n",
    "\n",
    "\n",
    "hospital_train_parquet = Input(\n",
    "    type=\"uri_file\", path=\"data/train_dataset.parquet\", mode=\"download\"\n",
    ")\n",
    "\n",
    "hospital_test_parquet = Input(\n",
    "    type=\"uri_file\", path=\"data/test_dataset.parquet\", mode=\"download\"\n",
    ")\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=compute_name,\n",
    "    description=\"Register Model for RAI hospital \",\n",
    "    experiment_name=f\"RAI_hospital_Model_Training_{model_name_suffix}\",\n",
    ")\n",
    "def my_training_pipeline(target_column_name, training_data):\n",
    "    trained_model = train_component_definition(\n",
    "        target_column_name=target_column_name,\n",
    "        training_data=training_data\n",
    "    )\n",
    "    trained_model.set_limits(timeout=3600)\n",
    "\n",
    "    _ = register_component(\n",
    "        model_input_path=trained_model.outputs.model_output,\n",
    "        model_base_name=model_base_name,\n",
    "        model_name_suffix=model_name_suffix,\n",
    "    )\n",
    "\n",
    "    return {}\n",
    "\n",
    "model_registration_pipeline_job = my_training_pipeline(target_column, hospital_train_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525742369
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import PipelineJob\n",
    "import webbrowser\n",
    "\n",
    "def submit_and_wait(ml_client, pipeline_job) -> PipelineJob:\n",
    "    created_job = ml_client.jobs.create_or_update(pipeline_job)\n",
    "    assert created_job is not None\n",
    "\n",
    "    while created_job.status not in ['Completed', 'Failed', 'Canceled', 'NotResponding']:\n",
    "        time.sleep(30)\n",
    "        created_job = ml_client.jobs.get(created_job.name)\n",
    "        print(\"Latest status : {0}\".format(created_job.status))\n",
    "\n",
    "\n",
    "    # open the pipeline in web browser\n",
    "    webbrowser.open(created_job.services[\"Studio\"].endpoint)\n",
    "    \n",
    "    #assert created_job.status == 'Completed'\n",
    "    return created_job\n",
    "\n",
    "# This is the actual submission\n",
    "training_job = submit_and_wait(ml_client, model_registration_pipeline_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525742954
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "expected_model_id = f'{model_base_name}_{model_name_suffix}:1'\n",
    "azureml_model_id = f'azureml:{expected_model_id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525743247
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def get_categorical_numerical_data(dataset):\n",
    "    dataset = dataset.drop([target_column], axis = 1)  \n",
    "    categorical = []\n",
    "    for col, value in dataset.iteritems():\n",
    "        if value.dtype == 'object' or value.dtype == 'bool':\n",
    "            categorical.append(col)\n",
    "    numerical = dataset.columns.difference(categorical)\n",
    "    return categorical, numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525743514
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# get categorical and numerical fields from training data\n",
    "categorical, numerical = get_categorical_numerical_data(train_data)\n",
    "print(\"categorical columns: \",  categorical)\n",
    "print(\"numerical field: \", numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525743891
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "label = \"latest\"\n",
    "\n",
    "rai_constructor_component = ml_client_registry.components.get(\n",
    "    name=\"microsoft_azureml_rai_tabular_insight_constructor\", label=label\n",
    ")\n",
    "\n",
    "# We get latest version and use the same version for all components\n",
    "version = rai_constructor_component.version\n",
    "\n",
    "rai_counterfactual_component = ml_client_registry.components.get(\n",
    "    name=\"microsoft_azureml_rai_tabular_counterfactual\", version=version\n",
    ")\n",
    "\n",
    "rai_causal_component = ml_client_registry.components.get(\n",
    "    name=\"microsoft_azureml_rai_tabular_causal\", version=version\n",
    ")\n",
    "\n",
    "rai_explanation_component = ml_client_registry.components.get(\n",
    "    name=\"microsoft_azureml_rai_tabular_explanation\", version=version\n",
    ")\n",
    "\n",
    "rai_erroranalysis_component = ml_client_registry.components.get(\n",
    "    name=\"microsoft_azureml_rai_tabular_erroranalysis\", version=version\n",
    ")\n",
    "\n",
    "rai_gather_component = ml_client_registry.components.get(\n",
    "    name=\"microsoft_azureml_rai_tabular_insight_gather\", version=version\n",
    ")\n",
    "\n",
    "rai_scorecard_component = ml_client_registry.components.get(\n",
    "    name=\"microsoft_azureml_rai_tabular_score_card\", version=version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525744212
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "score_card_config_dict = {\n",
    "    \"Model\": {\n",
    "        \"ModelName\": \"Diabetes hospital readmission\",\n",
    "        \"ModelType\": \"Classification\",\n",
    "        \"ModelSummary\": \"This model predicts whether a diabetic patient will be readmitted back to hospital within 30 days\"\n",
    "    },\n",
    "    \"Metrics\" :{\n",
    "        \"accuracy_score\": {\n",
    "            \"threshold\": \">=0.80\"\n",
    "        },\n",
    "        \"precision_score\": {}\n",
    "    },\n",
    "    \"FeatureImportance\": { \n",
    "        \"top_n\": 10 \n",
    "    }, \n",
    "    \"DataExplorer\": { \n",
    "        \"features\": [ \n",
    "        \"time_in_hospital\", \n",
    "        \"prior_inpatient\"\n",
    "        ] \n",
    "    }\n",
    "}\n",
    "\n",
    "score_card_config_filename = \"rai_hospital_readmission_score_card_config.json\"\n",
    "\n",
    "with open(score_card_config_filename, 'w') as f:\n",
    "    json.dump(score_card_config_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684525744432
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "score_card_config_path = Input(\n",
    "    type=\"uri_file\",\n",
    "    path=score_card_config_filename,\n",
    "    mode=\"download\"\n",
    ")\n",
    "\n",
    "@dsl.pipeline(\n",
    "        compute=compute_name,\n",
    "        description=\"RAI computation on hospital readmit classification data\",\n",
    "        experiment_name=f\"RAI_hospital_Classification_RAIInsights_Computation_{model_name_suffix}\",\n",
    "    )\n",
    "def rai_classification_pipeline(\n",
    "        target_column_name,\n",
    "        training_data,\n",
    "        testing_data,\n",
    "        score_card_config_path\n",
    "    ):\n",
    "        \n",
    "        # Initiate the RAIInsights\n",
    "        create_rai_job = rai_constructor_component(\n",
    "            title=\"RAI Dashboard\",\n",
    "            task_type=\"classification\",\n",
    "            model_info=expected_model_id,\n",
    "            model_input=Input(type=AssetTypes.MLFLOW_MODEL, path=azureml_model_id),            \n",
    "            train_dataset=training_data,\n",
    "            test_dataset=testing_data,\n",
    "            target_column_name=target_column_name,\n",
    "            #classes=json.dumps(['not readmitted', 'readmitted']),\n",
    "            categorical_column_names=json.dumps(categorical),\n",
    "        )\n",
    "        create_rai_job.set_limits(timeout=3600)\n",
    "        \n",
    "        # Add an explanation\n",
    "        explain_job = rai_explanation_component(\n",
    "            comment=\"Explanation for hospital remitted less than 30days  classification\",\n",
    "            rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,\n",
    "        )\n",
    "        explain_job.set_limits(timeout=3600)\n",
    "        \n",
    "        # Add error analysis\n",
    "        erroranalysis_job = rai_erroranalysis_component(\n",
    "            rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,\n",
    "        )\n",
    "        erroranalysis_job.set_limits(timeout=3600)\n",
    "\n",
    "        # Add counterfactual analysis\n",
    "        counterfactual_job = rai_counterfactual_component(\n",
    "            rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,\n",
    "            total_cfs=10,\n",
    "            desired_class='opposite',\n",
    "        )\n",
    "        counterfactual_job.set_limits(timeout=3600)\n",
    "\n",
    "        # Add causal analysis\n",
    "        causal_job = rai_causal_component(\n",
    "            treatment_features=json.dumps(['time_in_hospital']),\n",
    "            rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,\n",
    "        )\n",
    "        causal_job.set_limits(timeout=3600)\n",
    "        \n",
    "\n",
    "        # Combine everything\n",
    "        rai_gather_job = rai_gather_component(\n",
    "            constructor=create_rai_job.outputs.rai_insights_dashboard,\n",
    "            insight_1=explain_job.outputs.explanation,\n",
    "            insight_2=causal_job.outputs.causal,\n",
    "            insight_3=counterfactual_job.outputs.counterfactual,\n",
    "            insight_4=erroranalysis_job.outputs.error_analysis,\n",
    "        )\n",
    "        rai_gather_job.set_limits(timeout=3600)\n",
    "\n",
    "        rai_gather_job.outputs.dashboard.mode = \"upload\"\n",
    "        rai_gather_job.outputs.ux_json.mode = \"upload\"\n",
    "\n",
    "        # Generate score card in pdf format for a summary report on model performance,\n",
    "        # and observe distrbution of error between prediction vs ground truth.\n",
    "        rai_scorecard_job = rai_scorecard_component(\n",
    "            dashboard=rai_gather_job.outputs.dashboard,\n",
    "            pdf_generation_config=score_card_config_path\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"dashboard\": rai_gather_job.outputs.dashboard,\n",
    "            \"ux_json\": rai_gather_job.outputs.ux_json,\n",
    "            \"scorecard\": rai_scorecard_job.outputs.scorecard\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684526231989
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from azure.ai.ml import Output\n",
    "\n",
    "# Pipeline to construct the RAI Insights\n",
    "insights_pipeline_job = rai_classification_pipeline(\n",
    "    target_column_name=target_column,\n",
    "    training_data=hospital_train_parquet,\n",
    "    testing_data=hospital_test_parquet,\n",
    "    score_card_config_path=score_card_config_path,\n",
    ")\n",
    "\n",
    "# Workaround to enable the download\n",
    "rand_path = str(uuid.uuid4())\n",
    "insights_pipeline_job.outputs.dashboard = Output(\n",
    "    path=f\"azureml://datastores/workspaceblobstore/paths/{rand_path}/dashboard/\",\n",
    "    mode=\"upload\",\n",
    "    type=\"uri_folder\",\n",
    ")\n",
    "insights_pipeline_job.outputs.ux_json = Output(\n",
    "    path=f\"azureml://datastores/workspaceblobstore/paths/{rand_path}/ux_json/\",\n",
    "    mode=\"upload\",\n",
    "    type=\"uri_folder\",\n",
    ")\n",
    "insights_pipeline_job.outputs.scorecard = Output(\n",
    "    path=f\"azureml://datastores/workspaceblobstore/paths/{rand_path}/scorecard/\",\n",
    "    mode=\"upload\",\n",
    "    type=\"uri_folder\",\n",
    ")\n",
    "\n",
    "# submit pipeline\n",
    "insights_job = submit_and_wait(ml_client, insights_pipeline_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1684526232207
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "sub_id = ml_client._operation_scope.subscription_id\n",
    "rg_name = ml_client._operation_scope.resource_group_name\n",
    "ws_name = ml_client.workspace_name\n",
    "\n",
    "expected_uri = f\"https://ml.azure.com/model/{expected_model_id}/model_analysis?wsid=/subscriptions/{sub_id}/resourcegroups/{rg_name}/workspaces/{ws_name}\"\n",
    "\n",
    "print(f\"Please visit {expected_uri} to see your analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
